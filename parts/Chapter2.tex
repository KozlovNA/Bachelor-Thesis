\section{Крыловские методы решения систем уравнений}
\label{sec:Chapter2} \index{Chapter2}
Ключевым объектом в рассматриваемом классе методов является пространство Крылова, определим его.
% \newtheorem{definition}{Определение}
\begin{definition}
    Пусть $A$ - матрица порядка $N$, $v$ - вектор размерности $N$. Тогда линейная
    оболочка вида $K_m\left(A,v\right) \equiv \{v,Av,A^2v,...,A^{m-1}v\} $ называется 
    подпространством Крылова, где $m$ - натуральное число. 
\end{definition}
Все рассматриваемые в дальнейшем методы являются проекционными.
В таких методах приближенное решение ищется в крыловском пространстве при этом 
решение на подпространстве ищется, как правило, на основе некоторого проекционного
 соотношения (которое и задаёт метод).
\subsection[Процедура Арнольди]{Процедура Арнольди \cite{Saad2003}}
Процедура Арнольди - это алгоритм построения ортогонального базиса в крыловском
подпространстве $K_m$. Алгоритм \ref{alg:arnoldi} представляет наиболее простую
вариацию такого алгоритма в точной арифметике.
\begin{algorithm}[H]
    \caption{Алгоритм Арнольди}\label{alg:arnoldi}
    \begin{algorithmic}[1]
    \State Выберем $v_1 = v / \|v\|_2$, так что $\|v_1\|_2 = 1$
    \Statex
    \For{$j = 1, 2, \ldots, m$}
        \For{$i = 1, 2, \ldots, j$}
            \State $h_{ij} \gets (Av_j, v_i)$
        \EndFor
        \State $w_j \gets Av_j - \sum_{i=1}^j h_{ij}v_i$
        \State $h_{j+1,j} \gets \|w_j\|_2$
        \If{$h_{j+1,j} = 0$}
            \State \textbf{Stop}
        \EndIf
        \State $v_{j+1} \gets w_j/h_{j+1,j}$
    \EndFor
    \end{algorithmic}
\end{algorithm}
Алгоритм на каждом шаге ортогонализует $Av_j$ ко всем предыдущим $v_i$, 
применяя процедуру Грама-Шмидта. 
\par Результат работы алгоритма можно записать в матричном
виде: обозначим $V_m$ - $N \times m$ матрицу со столбцами $v_1,...,v_m$; 
$\overline{H}_m$ - $\left(m+1\right) \times \left( m \right)$  хессенбергова матрица с 
элементами $h_{ij}$ из алгоритма \ref{alg:arnoldi}; ${H}_m$ - $ m \times m $ матрица, 
получающаяся из $\overline{H}_m$ путем удаления последней строки. Тогда, процедура Арнольди
влечет следующие соотношения:
\begin{align}
    AV_m &= V_m H_m + w_m e_m^T \\
         &= V_{m+1} \overline{H}_m,\\
    V_m^T A V_m &= H_m \label{eq:VAVHM}
\end{align} 
\subsection[Симметричный алгоритм Ланцоша]{Симметричный алгоритм Ланцоша \cite{Saad2003}}
Симметричный алгоритм Ланцоша - это частный случай процедуры Арнольди, когда матрица 
$A$ - симметричная, при таком условии хессенбергова матрица $H_m$ становится симметричной
тридиагональной $T_m$. Это позволяет получить короткие рекуррентные соотношения, 
приведённые в Алгоритме \ref{alg:lanczos} 
\begin{algorithm}
    \caption{Симметричный алгоритм Ланцоша}\label{alg:lanczos}
    \begin{algorithmic}[1]
    \State Выберем $v_1 = v / \|v\|_2$, так что $\|v_1\|_2 = 1$
    \State $\beta_1 \gets 0$, $v_0 \gets 0$
    \For{$j = 1, 2, \ldots, m$}
        \State $w_j \gets Av_j - \beta_j v_{j-1}$
        \State $\alpha_j \gets (w_j, v_j)$
        \State $w_j \gets w_j - \alpha_j v_j$
        \State $\beta_{j+1} \gets \|w_j\|_2$
        \If{$\beta_{j+1} = 0$}
            \State \textbf{Stop}
        \EndIf
        \State $v_{j+1} \gets w_j / \beta_{j+1}$
    \EndFor
    \end{algorithmic}
    \end{algorithm} 

При этом матрица $T_m$ имеет вид: 
\begin{equation}
T_m = \begin{pmatrix}
\alpha_1 & \beta_2 & & & \\
\beta_2 & \alpha_2 & \beta_3 & & \\
& \beta_3 & \ddots & \ddots & \\
& & \ddots & \alpha_{m-1} & \beta_m \\
& & & \beta_m & \alpha_m
\end{pmatrix}
\label{eq:T_m}
\end{equation}

\subsection[Метод сопряженных градиентов]{Метод сопряженных градиентов \cite{Saad2003}}
Симметричный алгоритм Ланцоша можно использовать для итеративного решения систем
линейных уравнений с симметричной положительно определенной матрицей.
\par Пусть задано начальное приближение $x_0$, и векторы направлений из алгоритма
Ланцоша $v_i,\;i=1,...,m$.
На $m$-ом шаге алгоритма приближенное решение ищется в аффинном пространстве $x_0 + K_m$, 
где $K_m\left(A,r_0\right) \equiv \{r_0,Ar_0,A^2r_0,...,A^{m-1}r_0\}, \; r_0 = b - Ax_0$.
На невязки при этом налагается условие 
\begin{equation}
    \label{eq:galerkin}
    b-Ax_m \perp K_m.
\end{equation}
Если взять $v_1 = r_0/\|r_0\|_2$ и 
обозначить $\beta = \|r_0\|_2$. Тогда $V_m^TAV_m = T_m$ из \eqref{eq:VAVHM}, а также $V_m^Tr_0 = V_m^T(\beta v_1)=\beta e_1$.
Разложим приближенное решение на $m$-ом шаге по базису из векторов $v_i,\;i=1,...,m$:
\begin{equation}
    x_m = x_0 + V_m y_m.
\end{equation} 
Это выражение эквивалентно равенству:
 \begin{equation}
    r_m = r_0 - AV_m y_m, 
 \end{equation}
домножим слева на $V_m^T$:
\begin{equation}
    \label{eq:VTreqVTr0VAVy}
    V_m^T r_m = V_m^T r_0 - V_m^T AV_m y_m.
\end{equation}
Из \eqref{eq:galerkin} следует, что $ V_m^T r_m = 0 $, учтём это в \eqref{eq:VTreqVTr0VAVy} 
и выразим $y_m$:
\begin{equation}
    y_m = T_m^{-1} \beta e_1.
\end{equation}

\par Получим выражение для $r_m$:
\begin{align*}
    r_m &= b - A(x_0 + V_m y_m) \\
        &= r_0 - AV_m y_m \\
        &= \beta v_1 - (V_m T_m + t_{m+1,m}v_{m+1}e_m^T)y_m \\
        &= V_m \underbrace{(\beta e_1 - T_m y_m)}_{=0} - t_{m+1,m} e_m^T y_m v_{m+1}
\end{align*}
\begin{equation}
    \label{eq:r_m_SL}
    r_m = - t_{m+1,m} e_m^T y_m v_{m+1}.
\end{equation}
Из этого выражения следует, что $r_m \parallel v_{m+1}$, а значит, что невязки на каждом шаге ортогональны друг другу.

\par Получим короткие итерационные соотношения для обновления приближенного решения $x_m$.
LU-разложение матрицы $T_m$:
\begin{equation*}
    T_m = L_m U_m = 
        \begin{pmatrix}
        1 & & & & \\
        \lambda_2 & 1 & & & \\
        & \lambda_3 & \ddots & & \\
        & & \ddots & 1 &  \\
        & & & \lambda_m & 1
        \end{pmatrix}
        \begin{pmatrix}
            \eta_1 & \beta_2 & & & \\
             & \eta_2 & \beta_3 & & \\
            & & \ddots & \ddots & \\
            & & & \eta_{m-1} & \beta_m \\
            & & & & \eta_m
        \end{pmatrix}
\end{equation*} 
Введем обозначения
\begin{align}
    P_m &\equiv V_m U_m^{-1}, \label{eq:P_m}\\
    z_m &\equiv L_m^{-1} \beta e_1,
\end{align}
тогда приближенное решение выражается как
\begin{equation}
    x_m = x_0 + P_m z_m.
\end{equation}
Используя равенство \eqref{eq:P_m} получим формулу для обновления $p_m$-последнего
столбца $p_m$ матрицы $P_m$
\begin{align}
    &P_m U_m = V_m \\
    &p_m \eta_m + \beta_m p_{m-1} = v_m \\
    &p_m = \eta_m^{-1} \left( v_m - \beta_m p_{m-1} \right) \label{eq:p_m_update}
\end{align}
Выразим элементы из последней строчки матрицы $T_m$ с помощью LU-разложения:
\begin{align*}
    \alpha_m = \lambda_m \beta_m + \eta_m &\implies \eta_m = \alpha_m - \lambda_m \beta_m \\
    \beta_m = \lambda_m \eta_{m-1} &\implies \lambda_m = \beta_m / \eta_{m-1}
\end{align*}
В силу вида матрицы $L_m$:
\begin{align*}
    z_m &= 
    \begin{pmatrix}
        z_{m-1} \\
        \zeta_m
    \end{pmatrix}, \\
    \zeta_m &= -\lambda_m \zeta_{m-1}. 
\end{align*}
Как результат получаем формулу для обновления $x_m$:
\begin{equation*}
    x_m = x_{m-1} + \zeta_m p_m
\end{equation*}
Покажем, что столбцы $P_m$ образуют А-ортогональную систему, т.е, что $(Ap_i,p_j) = 0$, для $i \neq j$.
Для этого нужно показать, что $P_m^T AP_m$ - диагональная матрица. Подставим определение $P_m$ в это выражение:
\begin{align}
    P_m^T AP_m &= U_m^{-T}V_m^T AV_m U_m^{-1} \\
               &= U_m^{-T}T_m U_m^{-1} \\
               &=U_m^{-T}L_m
\end{align}
$U_m^{-T}L_m$ - нижнетреугольная матрица, но она также является и симметричной, 
так как $P_m^T AP_m$ - симметричная матрица. Таким образом, $U_m^{-T}L_m$ - диагональная матрица.
\par Следствием этого является то, что обновлять приближенное решение можно исходя из
поддержания свойств ортогональности невязок и А-ортогональности векторов направлений $p_i$.
В последующий выкладках вектора $p_j$ будут нумероваться с нуля, а не с единицы, как это было раньше.
А также коэффициенты будут переименованы, чтобы соответствовать общепринятым обозначениям.
\begin{align*}
    x_{j+1} = x_j + \alpha_j p_j \implies r_{j+1} = r_j - \alpha_j A p_j \\
    \alpha_j = \left( r_j, r_j \right) / \left( Ap_j, r_j \right)
\end{align*}
Из уравнения \eqref{eq:p_m_update} после перенормировки $p_i, i=1,...,m$ следует, что 
\begin{align*}
    &p_{j+1} = r_{j+1} + \beta_j p_j  \\
    &\beta_j = - (r_{j+1}, Ap_j) / (p_j, Ap_j) = \frac{1}{\alpha_j} (r_{j+1}, (r_{j+1}-r_j)) / (Ap_j,p_j) = (r_{j+1}, r_{j+1}) / (r_j,r_j)
\end{align*}
Это выражение и А-ортогональность $p_j$ в свою очередь можно использовать, чтобы преобразовать выражение для $\alpha_j$:
\begin{align*}
    &(Ap_j,r_j) = (Ap_j,p_j-\beta_{j-1}p_{j-1}) = (Ap_j,p_j) \\
    &\alpha_j = (r_j,r_j)/(Ap_j, p_j)
\end{align*}
Теперь у нас есть всё, чтобы записать алгоритм.
\begin{algorithm}
    \caption{Метод сопряженных градиентов}
    \begin{algorithmic}[1]
    \State $r_0 \gets b - A x_0$, $p_0 \gets r_0$.
    \For{$j = 0, 1, \ldots$}
        \State $\alpha_j \gets (r_j, r_j) / (A p_j, p_j)$
        \State $x_{j+1} \gets x_j + \alpha_j p_j$
        \State $r_{j+1} \gets r_j - \alpha_j A p_j$
        \State $\beta_j \gets (r_{j+1}, r_{j+1}) / (r_j, r_j)$
        \State $p_{j+1} \gets r_{j+1} + \beta_j p_j$
    \EndFor
    \end{algorithmic}
\end{algorithm}

Этот метод можно адаптировать и для систем общего вида, если домножить обе части уравнения
$Ax=b$ на $A^T$, и решать систему с симметричной положительно определенной матрицей $A^TA$,
однако число обусловленности при этом возрастает в квадрат раз из-за чего данный вариант может 
давать плохие результаты.
 
\subsection[Процесс биортогонализации Ланцоша]{Процесс биортогонализации Ланцоша \cite{Saad2003}}
Для несимметричных систем можно предъявить алгоритм похожий на симметричный алгоритм Ланцоша,
но который будет строить не ортогональный базис в пространстве Крылова, а пару биортогональных
базисов в пространствах $K_m(A, v_1) = span\{v_1,Av_1,...,A^{m-1}v_1\}$ и $K_m(A^T, v_1) = span\{v_1,A^Tv_1,...,(A^T)^{m-1}v_1\}$, 
то есть такую пару $v_1,...,v_m$ и $w_1,...,w_m$, что $(v_i,w_j)=\delta_{ij},\;1\leq i,\;j \leq m$.
\begin{algorithm}
    \caption{Процесс биортогонализации Ланцоша}
    \begin{algorithmic}[1]
    \State Выберем $v_{1}, w_{1}$ такие что $(v_{1}, w_{1}) = 1$.
    \State $\beta_{1} = \delta_{1} \equiv 0$, $w_{0} = v_{0} \equiv 0$
    \For{$j = 1, 2, \ldots, m$}
        \State $\alpha_{j} = (A v_{j}, w_{j})$
        \State $\hat{v}_{j+1} = A v_{j} - \alpha_{j} v_{j} - \beta_{j} v_{j-1}$
        \State $\hat{w}_{j+1} = A^{T} w_{j} - \alpha_{j} w_{j} - \delta_{j} w_{j-1}$
        \State $\delta_{j+1} = |(\hat{v}_{j+1}, \hat{w}_{j+1})|^{1/2}$
        \If{$\delta_{j+1} = 0$}
            \State Stop
        \EndIf
        \State $\beta_{j+1} = (\hat{v}_{j+1}, \hat{w}_{j+1}) / \delta_{j+1}$
        \State $v_{j+1} = \hat{v}_{j+1} / \beta_{j+1}$
        \State $w_{j+1} = \hat{w}_{j+1} / \delta_{j+1}$
    \EndFor
    \end{algorithmic}
    \end{algorithm}

Введём обозначения: 
\begin{equation*}
    T_m = 
    \begin{pmatrix}
        \alpha_1 & \beta_2 & & & \\
        \delta_2 & \alpha_2 & \beta_3 & & \\
        & \delta_3 & \ddots & \ddots & \\
        & & \ddots & \alpha_{m-1} & \beta_m \\
        & & & \delta_m & \alpha_m
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    W_m = \begin{bmatrix}
            w_1 & ... & w_m
        \end{bmatrix}
\end{equation*}

Тогда легко убедиться, что если на $m$-ом шаге не произошло аварийной остановки, 
то алгоритм порождает следующие матричные соотношения:
\begin{align*}
    AV_m &= V_m T_m + \delta_{m+1} v_{m+1} e_m^T\\
    A^T W_m &= W_m T^T_m + \beta_{m+1} w_{m+1} e_m^T \\
    W_m^T A V_m &= T_m  
\end{align*}

\subsection[Метод бисопряженных градиентов]{Метод бисопряженных градиентов \cite{Saad2003}}
\par Метод бисопряженных градиентов выводится из процесса биортогонализации Ланцоша
аналогично тому, как метод сопряженных градиентов выводился из симметричного процесса
Ланцоша. Прибилиженное решение на $m$-ом шаге будет искаться как наилучшее приближение в 
пространстве $x_0 + K_m$, где $K_m = \{v_1, Av_1,...,A^{m-1}v_1\}$, так, чтобы невязка 
$r_m$ была ортогональна пространству $L_m = {w_1,A^T w_1, ... ,(A^T)^{m-1} w_1}$. Так же, 
как и при выводе сопряженных градиентов возьмём $v_1 = r_0 / \| r_0 \|_2$, а вектор 
$w_1$ можно взять произвольным, таким что $(v_1, w_1) \neq 0$, например, $v_1$. Алгоритм
будет решать не только систему $Ax=b$, но и некоторую двойственную систему $A^T x^* = b^*$ (причём
 $b^*=b$, если $w_1 = v_1$).
Производим LU-разложение для матрицы $T_m$, полученной в ходе процесса биортогонализации Ланцоша:
\begin{equation*}
    T_m = L_m U_m,
\end{equation*}
и вводим обозначения для векторов направлений $p_i$, $p_i^*$:
\begin{align*}
    P_m &= V_m U_m^{-1} \\
    P_m^* &= W_m L_m^{-T} \\
    \begin{bmatrix} p_1 & ... & p_m \end{bmatrix} &= P_m \\
    \begin{bmatrix} p_1^* & ... & p_m^* \end{bmatrix} &= P_m^* \\
\end{align*} 
Приближенное решение выражается также как и в методе сопряженных градиентов:
\begin{equation*}
    x_m = x_0 + P_m L_m^{-1} (\beta e_1).
\end{equation*}
И, аналогично методу сопряженных градиентов невязки окажутся сонаправлены векторам из базиса:
\begin{align*}
    r_j &\parallel v_{j+1},\;j=1,...,m \\
    r_j^* &\parallel v_{j+1}^*,\;j=1,...,m
\end{align*}
Отсюда следует, что эти наборы невязок биортогональны:
\begin{equation}
    \label{eq:rjrieq0}
    (r_j^*,r_i) = 0,\;\text{при}\;1 \leq i,j \leq m,\;i \neq j
\end{equation}
% Введём обозначения для невязки на $m$-ой итерации и для двойственной невязки соответственно:
% \begin{align*}
%     r_m &= b - Ax_m \\
%     r_m^* &= b^* - A^T x_m^*
% \end{align*} 
Легко показать, что наборы векторов $p_i^*,\;i=1,...,m$ и $p_i,\;i=1,...,m$ - A-ортогональны:
\begin{equation*}
    (P_m^*)^T A P_m = L_m^{-1} W_m^T A V_m U_m^{-1} = L_m^{-1} T_m U_m^{-1} = I.
\end{equation*}
Благодаря полученнным свойствам биортогональности невязок и А-биортогональности
векторов направлений, аналогичным же образом можно получить короткие итерационные соотношения для получения новых векторов 
$p_i$, $r_i$ , $x_i$, и записать окончательный алгоритм:
\begin{algorithm}
    \caption{Метод бисопряженных градиентов}
    \begin{algorithmic}[1]
    \State $r_0 \gets b - A x_0$, $r_0^*$ т.ч. $(r_0^*,r_0) \neq 0$, например $r_0^*=r_0$ 
    \State $p_0 \gets r_0$, $p_0^*=r_0^*$
    \For{$j = 0, 1, \ldots$}
        \State $\alpha_j \gets (r_j^*, r_j) / (p_j^*, A p_j)$
        \State $x_{j+1} \gets x_j + \alpha_j p_j$
        \State $r_{j+1} \gets r_j - \alpha_j A p_j$
        \State $r_{j+1}^* \gets r_j^* - \alpha_j A^T p_j^*$
        \State $\beta_j \gets (r_{j+1}^*, r_{j+1}) / (r_j^*, r_j)$
        \State $p_{j+1} \gets r_{j+1} + \beta_j p_j$
        \State $p_{j+1}^* \gets r_{j+1} + \beta_j p_j^*$
    \EndFor
    \end{algorithmic}
\end{algorithm}
К сожалению, данный метод на практике проявляет нерегулярное уменьшение невязки и 
тратит вычислительные ресурсы на поиск решения двойственной задачи, которая нас не интересует.
Для решения этих проблем был придуман метод стабилизированных бисопряженных градиентов.

\subsection[Стабилизированный метод бисопряженных градиентов]{Стабилизированный метод бисопряженных градиентов \cite{doi:10.1137/0913035}}
Невязки, полученные при помощи метода бисопряженных градиентов $r_k$ и $r_k^*$ лежат в пространствах
Крылова $K_{m+1}^r(A, r_0) = \{r_0,A r_0, ... , A^m r_0\}$ и \\$K_{m+1}^l(A^T, r_0^*) = \{r_0^*,(A^T) r_0^*, ... , (A^T)^m r_0^*\}$ соответственно,
следовательно, их можно выразить с помощью многочлена от матрицы:
\begin{align*}
    r_k &= \mathcal{R}_k(A)r_0 \\
    r_k^* &= \mathcal{Q}_k(A^T)r_0^*
\end{align*}
Из вида итерационных соотношений легко видеть, что $\mathcal{R}_k \equiv \mathcal{Q}_k$,
 и $\mathcal{R}_k(0) = 1$. 

 \par Как было показано в предыдущем пункте, метод бисопряженных градиентов работает
 за счёт поддержания ортогонализационных соотношений на невязки и вектора направлений.
 Преобразуем соотношение для невязок \eqref{eq:rjrieq0}:
 \begin{align*}
    (r_j^*,r_i) &= 0 \\
    (\mathcal{Q}_j(A^T)r_0^*,\mathcal{R}_i(A)r_0) &= (r_0^*,\mathcal{Q}_j(A)\mathcal{R}_i(A)r_0) = 0 
 \end{align*}
 \begin{equation}
    \label{eq:bcgstaborthresiduals}
    (r_0^*,\mathcal{Q}_j(A)\mathcal{R}_i(A)r_0) = 0
 \end{equation}

Из этого же соотношения \eqref{eq:rjrieq0} следует, что $r_j \perp K^l_{j} (A^T, r_0^*)$, 
значит, выражение \eqref{eq:bcgstaborthresiduals} верно для любого многочлена $\mathcal{Q}_j$ порядка $j$.
В частности рассмотрим 
\begin{equation}
    \label{eq:bcgstabq}
    \mathcal{Q}_j(t) = (1-\omega_0 t)(1-\omega_1 t)\cdot...\cdot(1-\omega_{j-1} t)
\end{equation}
И будем выбирать $\omega_j$ так, чтобы минимизировать норму $r_i$.
Итерационные соотношения в методе бисопряженных градиентов можно записать в полиномиальном виде:
\begin{align}
    \mathcal{R}_{i+1} &= \mathcal{R}_i - t \alpha_i \mathcal{P}_i \\
    \mathcal{P}_{i+1} &= \mathcal{P}_i + \beta_i \mathcal{P}_i,
\end{align} 
где аналогично невязкам $r_i$ вектора направлений $p_i$ были выражены через 
полином от матрицы системы как $p_i = \mathcal{P}_i(A)r_0$. 

\par Опираясь на \eqref{eq:bcgstaborthresiduals} введём обозначение для стабилизированных
невязок: 
\begin{equation}
    \label{eq:bcgstabresstab}
    r_i = \mathcal{Q}_i(A)\mathcal{R}_i(A)r_0
\end{equation}
Получим короткие итерационные соотношения для обновления стабилизированной невязки:
\begin{align*}
    &\mathcal{Q}_{i+1}(A)\mathcal{R}_{i+1}(A)r_0  = (1-\omega_iA)\mathcal{Q}_{i}(A)(\mathcal{R}_i(A)-\alpha_i \mathcal{P}_i(A))r_0 = \\
    &= \{\mathcal{Q}_{i}(A)\mathcal{R}_{i}(A) - \alpha_i A \mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)\}r_0 -\omega_iA\{ \mathcal{Q}_{i}(A)\mathcal{R}_{i}(A) - \alpha_i A \mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)\ \}r_0
\end{align*}
Обозначим $s_i = r_i - \alpha_i A p_i$, тогда обновление стабилизированной невязки будет производится по следующему соотношению:
\begin{equation}
    \label{eq:bcgstabresupdate}
    r_{i+1} = s_i - \omega_i A s_i
\end{equation}
Выберем $\omega_i$ так, чтобы минимизировать норму невязки $r_{i+1}$:
\begin{equation}
    \omega_i = \frac{(As_i,s_i)}{(As_i,As_i)}
\end{equation}
Аналогично введем обозначение для новых векторов направлений:
\begin{equation}
    p_i = \mathcal{Q}_i(A) \mathcal{P}_i(A) r_0
\end{equation}
и получим рекуррентные соотношения для них:
\begin{align*}
    &\mathcal{Q}_{i+1}(A)\mathcal{P}_{i+1}(A)r_0 = \mathcal{Q}_{i+1}(A)(\mathcal{R}_{i+1}(A)+\beta_i\mathcal{P}_{i}(A))r_0 \\
    &= \mathcal{Q}_{i+1}(A)\mathcal{R}_{i+1}(A)r_0 + \beta_i\{\mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)r_0-\omega_iA\mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)\}r_0
\end{align*}
\begin{equation}
    \label{eq:bsgstabpupdate}
    p_{i+1} = r_{i+1} + \beta_i(p_i - \omega_i Ap_i)
\end{equation}
Теперь необходимо выразить коэффициенты $\beta_i$, $\alpha_i$ с помощью новых невязок и направлений. 
\begin{equation*}
    \tilde{\rho}_{i+1} \equiv (r_0^*, r_{i+1}) = (r_0^*, \mathcal{Q}_{i+1}(A)\mathcal{R}_{i+1}(A)r_0) = (\mathcal{Q}_{i+1}(A^T)r_0^*, \mathcal{R}_{i+1}(A)r_0)
\end{equation*} 
Как уже говорилось, для невязок, полученных при помощи метода бисопряженных градиентов справедливо, что 
$\mathcal{R}_j(A)r_0 \perp K^l_{j} (A^T, r_0^*)$, следовательно от $\mathcal{Q}_{i+1}(A^T)$ останется только 
$(-1)^{i}\omega_0 ... \omega_i (A^T)^i$. В методе бисопряженных градиентов $\mathcal{Q}_i \equiv \mathcal{R}_i$, 
следовательно,
\begin{equation*}
    \rho_{i+1} \equiv (\mathcal{R}_{i+1}(A^T)r_0^*, \mathcal{R}_{i+1}(A)r_0) = (-1)^i\alpha_0 \cdot ... \cdot \alpha_i ((A^T)^i,\mathcal{R}_{i+1})
\end{equation*} 
Таким образом, можно выразить $\beta_i$ через новые невязки и направления:
\begin{equation}
    \beta_i = \rho_{i+1} / \rho_i = (\alpha_i/\omega_i)(\tilde{\rho}_{i+1} / \tilde{\rho}_i) = \frac{(r_0^*,r_{i+1})}{(r_0^*,r_i)}\frac{\alpha_i}{\omega_i}
\end{equation}
Аналогичным образом можно показать, что
\begin{equation}
    \alpha_i = \frac{(r_0^*,r_i)}{(r_0^*,Ap_i)}
\end{equation}

Наконец, можно записать окончательный вид алгоритма:
\begin{algorithm}
    \caption{Стабилизированные бисопряженные градиенты}
    \begin{algorithmic}[1]
        \State $r_0 \gets b - A x_0$, $r_0^*$ т.ч. $(r_0^*,r_0) \neq 0$, например $r_0^*=r_0$ 
        \State $p_0 \gets r_0$, $p_0^*=r_0^*$
        \For{$k = 0, 1, \ldots$}
            \State $v_k \gets Ap_k$
            \State $\alpha_k \gets (r_0^*, r_k) / (r_0^*, v_k)$
            \State $s_k \gets r_k - \alpha_k v_k$
            \State $t_k \gets A s_k$
            \State $\omega_k = (t_k,s_k)/(t_k,t_k)$
            \State $x_{k+1} \gets x_k + \alpha_j p_j + \omega_k s_k$
            \State $r_{k+1} \gets r_k - \omega_k A t_k$
            \State $\beta_k \gets \frac{(r_0^*,r_{k+1})}{(r_0^*,r_k)}\frac{\alpha_k}{\omega_k}$
            \State $p_{k+1} \gets r_{k+1} + \beta_k (p_k - \omega_k v_k)$
        \EndFor
    \end{algorithmic}
\end{algorithm} 
 
\subsection[Блочный метод бисопряженных градиентов]{Блочный метод бисопряженных градиентов \cite{OLEARY1980293}}
Для решения линейных систем с многими правыми частям $AX=B$, где $A$ - невырожденная $N \times N$ матрица,
$B$ - $N \times s$ блок (матрица) правых частей с полным рангом,  
блочный метод бисопряженных градиентов строит 2 набора блоков направлений:
$\{ P_0, ... , P_k \}$ и $\{ \tilde{P}_0, ... , \tilde{P}_k \}$, чьи столбцы своей линейной оболочкой
порождают блочные пространства Крылова $K^r_{k+1}(A,R_0) = \{R_0,A R_0, ... ,A^k R_0\}$, где $R_0 = B - A X_0$ и
 $K^l_{k+1}(A^T,\tilde{R}_0) = \{\tilde{R}_0,A^T \tilde{R}_0, ... ,(A^T)^k \tilde{R}_0\}$, где $\tilde{R}_0$ - 
 произвольная $N \times s$ матрица (фигурные скобки обозначают линейную оболочку столбцов матриц в наборе), 
и два набора блоков невязок: $\{R_0, ... , R_k\}$ и $\{\tilde{R}_0, ... , \tilde{R}_k\}$ так, чтобы для них выполнялись 
блочные соотношения ортогональности:
\begin{align}
    &\tilde{R}_i^T R_j = 0, \text{ для } i < j \label{eq:bbcgorth1}\\
    &\tilde{P}_i^T A P_j = 0, \text{ для } i < j \label{eq:bbcgorth2}\\
    &R_i^T \tilde{R}_j = 0, \text{ для } i < j \label{eq:bbcgorth3}\\
    &P_i^T A \tilde{P}_j = 0, \text{ для } i < j \label{eq:bbcgorth4}\\
\end{align}
И выглядит он следующим образом:
\begin{algorithm}
    \caption{Блочный метод биспоряженных градиентов}
    \begin{algorithmic}
        \State $X_0$ - $N \times s$ блок начальных приближений, $R_0 = B - AX_0$
        \State $\tilde{R}_0$ - произвольная $ N\times s $ матрица
        \State $P_0 = R_0$, $\tilde{P}_0 = \tilde{R}_0$
        \For{$k=0,1,...$}
            \State $\alpha_k \gets (\tilde{P}_k^T A P_k)^{-1} \tilde{R}_k^T R_k$
            \State $X_{k+1} \gets X_k + P_k \alpha_k$
            \State $R_{k+1} \gets R_k - A P_k \alpha_k$
            \State $\tilde{\alpha}_k = (P_k^TA^T \tilde{P}_k)^{-1}R_k^T \tilde{R}_k$
            \State $\beta_k = (\tilde{R}_k^T R_k)^{-1} \tilde{R}_{k+1}^T R_{k+1}$
            \State $\tilde{\beta}_k = (R_k^T \tilde{R}_k)^{-1} R_{k+1}^T \tilde{R}_{k+1}$
            \State $P_{k+1} = R_{k+1} + P_k \beta_k$
            \State $\tilde{P}_{k+1} = \tilde{R}_{k+1} + \tilde{P}_k \tilde{\beta}_k$
        \EndFor 
    \end{algorithmic}
\end{algorithm}
алгоритм останавливается, если хотя бы одна из матриц: $\tilde{P}_k^T A P_k$ или $\tilde{R}_k^T R_k$ становится 
вырожденной. 

При $s=1$ приведённый метод эквивалентен бисопряженным градиентам. 

\subsection[Блочный метод сопряженных градиентов]{Блочный метод сопряженных градиентов \cite{OLEARY1980293}}
Если матрица $A$ - симметричная и положительно определенная и $\tilde{R}_0 = R_0$, 
то блочный метод бисопряженных градиентов превращается в блочный метод сопряженных градиентов:
\begin{algorithm}   
    \caption{Блочный метод сопряженных градиентов}
    \begin{algorithmic}
        \State $X_0$ - $N \times s$ блок начальных приближений, $R_0 = B - AX_0$
        \State $P_0 = R_0$
        \For{$k=0,1,...$}
            \State $\alpha_k \gets (P_k^T A P_k)^{-1} R_k^T R_k$
            \State $X_{k+1} \gets X_k + P_k \alpha_k$
            \State $R_{k+1} \gets R_k - A P_k \alpha_k$
            \State $\beta_k = (R_k^T R_k)^{-1} R_{k+1}^T R_{k+1}$
            \State $P_{k+1} = R_{k+1} + P_k \beta_k$
        \EndFor 
    \end{algorithmic}
\end{algorithm}
Важным свойством этого алгоритма является то, что для него существует теорема сходимости:
\begin{theorem}
    После  $k$ шагов блочного метода сопряженных градиентов, ошибка дял $i$-ой правой части $e^{(k)}_i = x_i^{(k)} - x_i^*$ ограничена как:
    \begin{equation}
        e^{(k)T}_i A e^{(k)}_i \leq \left( \frac{1 - \sqrt{\kappa^{-1}}}{1 + \sqrt{\kappa^{-1}}} \right)^{2k} c, 
    \end{equation}
    где $\kappa = \lambda_N/\lambda_s$, $c$ - некоторая константа.
\end{theorem}
Если решать линейную систему с многими правыми частями для каждой правой части в отдельности
методом сопряженных градиентов, то скорость сходимости будем определяться числом
обучловленности системы: $\kappa = \lambda_N / \lambda_1$. Если же решать блочным методом
сопряженных градиентов, то скорость сходимости определяется не нижней границей спектра $\lambda_1$,
а $s$-ой снизу компонентой $\lambda_s$, что может существенно повысить скорость сходимости. 

\subsection[Блочный стабилизированный метод бисопряженных градиентов]{Блочный стабилизированный метод бисопряженных градиентов \cite{elGuennouni2003}}
Блочное обобщение стабилизированных бисопряженных градиентов производится при помощи
 матричнозначных полиномов. Так что следует рассмотреть их определение и некоторые свойства. 
\subsubsection{Матричнозначные полиномы}
\begin{definition}
    \textit{Матричнозначным полиномом} $\mathcal{P}$ порядка $k$ называется полином вида:
    \begin{equation*}
        \mathcal{P}(t) = \sum_{i=0}^k t^i \Omega_i,
    \end{equation*} 
    где $\Omega_i$ - $s \times s$ матрица, а $t$ - число. 
\end{definition}
\begin{definition}[Операции с матричнозначными полиномами]
    \begin{align*}
        &\mathcal{P}(A) \circ Y = \sum_{i=0}^{k} A^i Y \Omega_i,\text{ где $Y$ - $N \times s$ матрица} \\
        &(\mathcal{P}\Theta)(t) = \sum_{i=0}^k t^i \Omega_i \Theta, \text{ где $\Theta$ - $s \times s$ матрица}
    \end{align*}
\end{definition}
\begin{preposition}[Свойства операций с матричнозначными полиномами]
    \label{prep:propoperpoly}
\begin{align*}
    & (\mathcal{P}(A) \circ Y) \Theta = (\mathcal{P} \Theta ) (A) \circ Y \\
    & (\mathcal{P} + \mathcal{Q})(A) \circ Y = \mathcal{P}(A) \circ Y + \mathcal{Q}(A) \circ Y
\end{align*}
\end{preposition}

 \subsubsection{Алгоритм}

Пользуясь свойствами из утверждения \ref{prep:propoperpoly}, аналогично методу бисопряженных градиентов можно выразить блок невязок и блок 
направлений с помощью матричнозначного полинома от матрицы системы:
\begin{align}
    &R_k = \mathcal{R}_k (A) \circ R_0 \label{eq:rkpolydef} \\
    &P_k = \mathcal{P}_k (A) \circ R_0 \label{eq:pkpolydef} \\
    &\mathcal{R}_{k+1}(t) = \mathcal{R}_k(t) - t \mathcal{P}(t)\alpha_k \label{eq:rkpolyrec}\\
    &\mathcal{P}_{k+1}(t) = \mathcal{R}_{k+1}(t) + \mathcal{P}(t)\beta_k \label{eq:pkpolyrec}\\
    &\mathcal{P}_0(t) = \mathcal{R}_0(t) = I_s  
\end{align}
Для того, чтобы записать свойства ортогональности в терминах матричнозначных 
полиномов полезно ввести определения двух функционалов:
\begin{definition}
    \begin{align}
        &C(\mathcal{P}) \equiv \tilde{R}_0^T (\mathcal{P}(A) \circ R_0) \\
        &C^{(1)}(\mathcal{P}) \equiv C(t\mathcal{P})
    \end{align}
\end{definition} 
Свойства ортогональности для блока невязок и блока направлений:
\begin{preposition}
    \label{prep:orthbcgstab}
    \begin{align}
        &C(\mathcal{R}_k \mathcal{T}_i) = 0, \text{ для $i < k$} \\
        &C^{(1)}(\mathcal{P}_k \mathcal{T}_i) = 0, \text{ для $i < k$}
    \end{align}
\end{preposition}
\begin{proof}
    Из свойства ортогональности для блочного метода бисопряженных градиентов \eqref{eq:bbcgorth1}
    следует, что
    $$R_k \perp K_k(A^T,\tilde{R}_0),$$
    следовательно, для $i < k$ 
    $$\tilde{R}_0^T A^i R_k = 0,$$
    и воспользуемся определением полиномов $\mathcal{R}_{k}$ \eqref{eq:rkpolydef}:
    $$\tilde{R}_0^T (A^i \mathcal{R}_k(A)) \circ R_0 = 0,$$
    откуда сразу же следует первое утверждение, которое требовалось доказать.
    По рекурентной формуле \eqref{eq:rkpolyrec}:
    $$AP_k=(R_k - R_{k+1})\alpha_k^{-1}.$$
    Применив это выражение, первое доказанное утверждение и определение \eqref{eq:pkpolydef}, получаем для $i < k$:
    $$C^{(1)}(t^{i}\mathcal{P}_k) = \tilde{R}_0^T (A^{i+1}\mathcal{P}_k(A) \circ R_0) = \tilde{R}_0^T A^{i+1} P_k = \tilde{R}_0^T A^i (R_k - R_{k+1})\alpha_k^{-1} = 0,$$
    откуда сразу следует второе утверждение, которое требовалось доказать.
\end{proof}
Рассмотрим матричнозначные полиномы $\mathcal{Q}_k$, которые задаются рекуретной формулой:
\begin{equation*}
    \mathcal{Q}_{k+1} = (1-\omega_k t)\mathcal{Q}_k(t),
\end{equation*}
где $\omega_k$ - скалярные матрица. И будем выбирать $\omega_k$ такой, чтобы минимизировать норму 
Фробениуса блока стабилизированных невязок, для которых введём переобозначение $R_{k} \equiv (\mathcal{Q}_k\mathcal{R}_k(A)) \circ R_0$, и для которых, аналогично стабилизированному методу бисопряженных градиентов, справедлива рекуррентная формула:
$$R_{k+1} =  S_k - \omega_k A S_k,$$
из которой следует, что $\omega_k$ выражается как
 $$\omega_k = \frac{<AS_k,S_k>_F}{<AS_k,AS_k>_F}.$$
Также введем переобозначение для стабилизированных направлений $P_k \equiv (\mathcal{Q}_k \mathcal{P}_k (A)) \circ R_0$.


Используя утверждение \ref{prep:orthbcgstab} и рекуретное соотношение \eqref{eq:rkpolyrec}, 
найдём выражения для $\alpha_k$:
$$C(\mathcal{Q}_k \mathcal{R}_k) = C^{(1)}(\mathcal{Q}_k\mathcal{P}_k)\alpha_k $$
Переводя в матричную запись, получаем линейную систему на $\alpha_k$:
$$(\tilde{R}_0^T AP_k)\alpha_k = \tilde{R}_0^TR_k$$
Аналагично, используем утверждение \ref{prep:orthbcgstab} и рекурретное соотношение \eqref{eq:pkpolyrec}, чтобы получить линейную систему на матрицу коэффициентов $\beta_k$:
$$C^{(1)}(\mathcal{Q}_k\mathcal{R}_{k+1}) = - C^{(1)}(\mathcal{Q}_k \mathcal{P}_k)\beta_k,$$
в матричном виде это выражение имеет вид:
$$(\tilde{R}_0^T A P_k)\beta_k=-\tilde{R}_0^T A S_k$$.

Итак, итоговый вид алгоритма:
\begin{algorithm}
    \caption{Блочный стабилизированный метод бисопряженных градиентов}
    \begin{algorithmic}[1]
        \State $X_0$ - блок начальных приближений, $R_0 = B - AX_0$ - блок начальных невязок
        \State $P_0 = R_0$
        \State $\tilde{R}_0$ - произвольная $N \times s$ матрица
        \For{$k=0,1,...$}
            \State $V_k = AP_k$
            \State $\alpha_k = (\tilde{R}_0^TV_k)^{-1}(\tilde{R}_0^T R_k)$
            \State $S_k = R_k - V_k \alpha_k$
            \State $T_k = A S_k$
            \State $\omega_k = \frac{<T_k,S_k>_F}{<T_k,T_k>_F}$
            \State $X_{k+1} = X_k + P_k \alpha_k + \omega_k S_k$
            \State $R_{k+1} = S_{k} - \omega_k T_k$
            \State $\beta_k = (\tilde{R}_0^T V_k)^{-1}(-\tilde{R}_0^T T_k)$
            \State $P_{k+1} = R_{k+1} + (P_k-\omega_kV_k)\beta_k$
        \EndFor
    \end{algorithmic}
\end{algorithm}

Однако этот метод, как будет показано в разделе \ref{sec:Chapter4}, данный метод 
не сходится с большим количеством правых частей, и показывает слабое уменьшение 
числа итераций с увеличением размера блока в задаче электромагнитного рассеяния 
\cite{stavtsev2009application}. В разделе \ref{sec:Chapter3} будут представлены
шаги для повышения устойчивости вычислений. 
%  \par Проблемы со сходимостью метода из \cite{elGuennouni2003}, демонстрация в 4 главе, решение проблемы в 3 главе.
\subsection[Блочный симметричный метод квазиминимальных невязок]{Блочный симметричный метод квазиминимальных невязок \cite{doi:10.1137/0917019}}
Определим квазискалярное произведение двух векторов над полем комплексных чисел
 $x$ и $y$ как $\langle x,y\rangle _Q=\sum_k x_k y_k$. По отношению к такому произведению
 комплексные симметричные матрицы будут самосопряженными. Рассматриваемый в этом подразделе метод 
 итеративно ищет решение линейной системы порядка $N$ с $s$ правыми частями $AX=B$, в случае, когда $A$ - 
 симметричная комплесная матрица.   
\subsubsection[Блочный симметричный алгоритм Ланцоша]{Блочный симметричный алгоритм Ланцоша}
Естественным образом можно получить блочное обобщение симметричного метода Ланцоша, 
аналогично тому, как блочный метод бисопряженных градиентов следовал из бисопряженных градиентов.
Далее $\alpha_k$,$\beta_k$ - $s \times s$ матрицы, $V_k$ и $\tilde{V}_k$ - $N \times s$ 
матрицы, 
\begin{algorithm}
    \caption{Блочный симметричный алгоритм Ланцоша}
    \begin{algorithmic}[1]
        \State $X_0$ - блок начальных приближений
        \State $R_0 = B - AX_0$
        \State $V_0 = 0$
        \State $\tilde{V}_1 = R_0$
        \For{$k=1,2,...$}
            \State $V_k, \beta_k \xleftarrow{\text{квази-} QR} \tilde{V}_k$ \label{state:quasiqrtildev}
            \State $\tilde{V}_{k+1} = AV_k - V_{k-1}\beta_k^T$
            \State $\alpha_k = V_k^T \tilde{V}_{k+1}$
            \State $\tilde{V}_{k+1} \gets \tilde{V}_{k+1} - V_k \alpha_k$
        \EndFor
    \end{algorithmic}
\end{algorithm}
В строке номер \ref{state:quasiqrtildev} производится квази-QR разложение, которое 
можно выполнить с помощью модернизированного процесса Грамма-Шмидта, но нужно все скалярные
произведения заменить на квазискалярные произведения. 

Результат работы блочного симметричного алгоритма Ланцоша можно записать в матричном
виде:
\begin{equation}
    \label{eq:bsqmrmatr}
A\mathcal{V}_k = \mathcal{V}_{k+1} \tilde{T}_k,
\end{equation}
где были введены обозначения:
\begin{equation*}
    \mathcal{V}_k = \begin{bmatrix} V_1, ... , V_k \end{bmatrix}
\end{equation*}
\begin{equation*}
    \tilde{T}_k = \begin{bmatrix}
                    \alpha_1 & \beta_2^T & & & \\
                    \beta_2 & \alpha_2 & \beta_3^T & & \\
                    & \beta_3 & \ddots & \ddots & \\
                    & & \ddots & \alpha_{k-1} & \beta_{k}^T \\
                    & & & \beta_k & \alpha_k \\
                    & & & & \beta_{k+1}
                  \end{bmatrix}.
\end{equation*}
% Причём столбцы $\mathcal{V}_k$ образуют базис в блочном пространстве Крылова 
% $$K$$ 
\subsubsection[Алгоритм]{Алгоритм}
Приближенное решение будем искать в виде $X_k = X_0 + \mathcal{V}_k Z_k$, тогда для 
блока невязок $R_k = B - AX_k$ после применения формулы \eqref{eq:bsqmrmatr} получаем выражение:
$$R_k = V_1 \beta_1 - \mathcal{V}_{k+1}\tilde{T}_k Z_k$$.
Введём обозначение $\Omega_k = Diag(\omega_1,...,\omega_k),$ где $\omega_i = Diag(\| col_i(V_k) \|)$.

Вставим $\Omega_k$ в формулу для $R_k$: 
$$R_k = \mathcal{V}_{k+1} \Omega_{k+1}^{-1} \left[ \omega_1 e_1 \beta_1 - \Omega_{k+1} \tilde{T}_k Z_k \right],$$
где $e_1$ - $N \times s$ матрица, которая состоит из первых $s$ строк единичной матрицы.  

Если бы $s$ была равна 1, и если бы $\mathcal{V}$ была бы матрицей с ортогональными столбцами, то по 
унитарной инвариантности нормы Фробениуса, можно было бы выразить норму невязки как:
$$\| \omega_1 e_1 \beta_1 - \Omega_{k+1} \tilde{T}_k Z_k \|,$$ 
и дальше можно было бы минимизировать эту норму. Но предположения, в которых полученно
данное выражение в нашем случае не имеют места. Однако все равно можно построить алгоритм,
который бы подбирал $Z_k$ такой, чтобы минимизировать эту норму, в этом и заключается идея "квазиминимальных невязок", а сама эта норма называется \textit{квазиневязкой}.

Таким образом, будем искать такую $Z_k$, чтобы минимизировать норму 
$$\| col_i(\omega_1 e_1 \beta_1 - \Omega_{k+1} \tilde{T}_k Z_k) \|$$
для каждой правой части $i=1,..,m$ независимо и одновременно. Минимизация осуществляется
путём QR разложения, где матрица $Q_{k+1}$ выбирается так, чтобы 
$$Q_{k+1} \Omega_{k+1} \tilde{T}_k = \begin{bmatrix}
                                        U_k \\
                                        0
                                     \end{bmatrix} =  \begin{bmatrix}
                                                          \zeta_1 & \eta_2 & \theta_3 & & \\
                                                          & \zeta_2 & \eta_3 & \ddots & \\
                                                          & & \ddots & \ddots & \theta_k \\
                                                          & & & \zeta_{k-1} & \eta_{k} \\
                                                          & & & & \zeta_k \\
                                                          & & & & 0
                                                      \end{bmatrix}$$ 
Рекуретное обновление матрицы $Q_{k+1}$:
$$Q_{k+1} = \begin{bmatrix}
                I_{(k-1)s} & 0 \\
                0          & Q(a_k,b_k,c_k,d_k)
            \end{bmatrix} \begin{bmatrix}
                                Q_k & 0 \\
                                0   & I_s \\ 
                            \end{bmatrix}, \text{ где } Q(a_k, b_k, c_k, d_k) = \begin{bmatrix}
                                                                                    a_k & b_k \\
                                                                                    c_k & d_k
                                                                                \end{bmatrix} $$

По унитарной инвариантности нормы Фробениуса, преобразуем квазиневязку:
$$\| col_i \left( Q_{k+1} \omega_1 e_1 \beta_1 - \begin{bmatrix} R_k \\ 0 \end{bmatrix} Z_k \right) \|, \text{ для } i=1,...,m$$
Чтобы минимизировать квазиневязку в таком виде, определим 
$$ \tilde{t}_{k+1} = Q_{k+1} \omega_1 e_1 \beta_1 = \begin{bmatrix} t_k \\ \tilde{\tau}_{k+1} \end{bmatrix}, \text{ где } \tilde{\tau}_{k+1} \text{ - это } s \times s \text{ матрица.} $$ 
и выбирем $Z_k$ как $Z_k = R_k^{-1} t_k$.

Тогда квазиневязку можно вычислить с помощью $\tilde{\tau}_{k+1}$:
$$QRES_k = max_i \| col_i (\tilde{\tau}_{k+1}) \|,$$
а настоящая невязка определяется, как и в других рассмотренных блочных методах:
$$RES_k = max_i \| col_i (AX_k - B) \|.$$

Итоговый вид алгоритма:
\begin{algorithm}
    \caption{Блочный симметричный метод квазиминимальных невязок}
    \begin{algorithmic}[1]
        \State $V_0 = P_{0} = P_{-1} = 0_{N \times s}$, $N$ - размер матрицы $A$, $s$ - количество правых частей.
        \State $c_0 = b_{-1} = b_0 = 0_{s \times s}$
        \State $a_0 = d_{-1} = d_0 = I_{s \times s}$
        \State $R_0 = B - AX_0$
        \State $\tilde{V}_1 = R_0$
        \State $V_1,\; \beta_1 \xleftarrow{квази-QR} \tilde{V}_1 $
        \State $\omega_0 = 0_{s\times s}$
        \State $\omega_1 = Diag\{ \| col_i (v_i) \| \}$
        \State $\tilde{\tau}_1 = \omega_1 \beta_1$
        \For {$k = 1, ... $}
            \State $\tilde{V}_{k+1} = AV_k - V_{k-1} \beta^T_{k}$
            \State $\alpha_k = V_k^T \tilde{V}_{k+1}$
            \State $\tilde{V}_{k+1} = \tilde{V}_{k+1} - V_k \alpha_k$
            \State $ V_{k+1},\; \beta_{k+1} \xleftarrow{квази QR} \tilde{V}_{k+1} $
            \State $\omega_{k+1} = Diag\{ \| col_i (v_{k+1}) \| \}$
            \State $\theta_k = b_{k-2} \omega_{k-1} \beta_k^T $
            \State $\eta_k = a_{k-1}d_{k-2} \omega_{k-1} \beta^T_{k} + b_{k-1} \omega_k \alpha_k$
            \State $\tilde{\zeta}_k = c_{k-1} d_{k-2} \omega_{k-1} \beta_{k}^T + d_{k-1} \omega_k \alpha_k$
            \State $ Q_k ,\; 
                    \begin{bmatrix}
                        \zeta_k \\
                        0_{s \times s}
                    \end{bmatrix} \xleftarrow{QR} \begin{bmatrix}
                                        \tilde{\zeta}_k \\
                                        \omega_{k+1} \beta_{k+1}
                                     \end{bmatrix}$
            \State $\begin{bmatrix}
                        a_k & b_k \\
                        c_k & d_k
                    \end{bmatrix} \gets Q_k^*$
            \State $P_k = (V_k - P_{k-1}\eta_k - P_{k-2} \theta_k)\zeta_k^{-1}$
            \State $\tau_k = a_k \tilde{\tau}_k$
            \State $X_k = X_{k-1} + P_{k} \tau_{k}$
            \State $\tilde{\tau}_{k+1} = c_k \tilde{\tau}_k$
        \EndFor
    \end{algorithmic}
\end{algorithm}

Однако, как будет показано в разделе \ref{sec:Chapter4} у этого метода есть ряд проблем:
\begin{enumerate}
    \item Возможен большой взлёт невязки на первых итерациях алгоритма.
    \item Поддерживается квази-ортогональность внутри блока $V_i^TV_i$ - диагональная матрица. Вместо этого свойства было бы лучше поддерживать другое, например, обычную ортогональность внутри блока.  
    \item Нет сходимости в арифметике с одинарной точностью в задаче электромагнитного рассеяния \cite{stavtsev2009application}, если брать все 722 правые части в блоке.
\end{enumerate}

\newpage
