\section{Крыловские методы решения систем уравнений}
\label{sec:Chapter2} \index{Chapter2}
Ключевым объектом в рассматриваемом классе методов является пространство Крылова, определим его.
\newtheorem{definition}{Определение}
\begin{definition}
    Пусть $A$ - матрица порядка $N$, $v$ - вектор размерности $N$. Тогда линейная
    оболочка вида $K_m\left(A,v\right) \equiv \{v,Av,A^2v,...,A^{m-1}v\} $ называется 
    подпространством Крылова, где $m$ - натуральное число. 
\end{definition}
Все рассматриваемые в дальнейшем методы являются проекционными.
В таких методах приближенное решение ищется в крыловском пространствепри этом 
решение на подпространстве ищется, как правило, на основе некоторого проекционного
 соотношения (которое и задаёт метод).
\subsection[Процедура Арнольди]{Процедура Арнольди \cite{Saad2003}}
Процедура Арнольди - это алгоритм построения ортогонального базиса в крыловском
подпространстве $K_m$. Алгоритм \ref{alg:arnoldi} представляет наиболее простую
вариацию такого алгоритма в точной арифметике.
\begin{algorithm}[H]
    \caption{Алгоритм Арнольди}\label{alg:arnoldi}
    \begin{algorithmic}[1]
    \State Выберем $v_1 = v / \|v\|_2$, так что $\|v_1\|_2 = 1$
    \Statex
    \For{$j = 1, 2, \ldots, m$}
        \For{$i = 1, 2, \ldots, j$}
            \State $h_{ij} \gets (Av_j, v_i)$
        \EndFor
        \State $w_j \gets Av_j - \sum_{i=1}^j h_{ij}v_i$
        \State $h_{j+1,j} \gets \|w_j\|_2$
        \If{$h_{j+1,j} = 0$}
            \State \textbf{Stop}
        \EndIf
        \State $v_{j+1} \gets w_j/h_{j+1,j}$
    \EndFor
    \end{algorithmic}
\end{algorithm}
Алгоритм на каждом шаге ортогонализует $Av_j$ ко всем предыдущим $v_i$, 
применяя процедуру Грама-Шмидта. 
\par Результат работы алгоритма можно записать в матричном
виде: обозначим $V_m$ - $N \times m$ матрицу со столбцами $v_1,...,v_m$; 
$\overline{H}_m$ - $\left(m+1\right) \times \left( m \right)$  хессенбергова матрица с 
элементами $h_{ij}$ из алгоритма \ref{alg:arnoldi}; ${H}_m$ - $ m \times m $ матрица, 
получающаяся из $\overline{H}_m$ путем удаления последней строки. Тогда, процедура Арнольди
влечет следующие соотношения:
\begin{align}
    AV_m &= V_m H_m + w_m e_m^T \\
         &= V_{m+1} \overline{H}_m,\\
    V_m^T A V_m &= H_m \label{eq:VAVHM}
\end{align} 
\subsection[Симметричный алгоритм Ланцоша]{Симметричный алгоритм Ланцоша \cite{Saad2003}}
Симметричный алгоритм Ланцоша - это частный случай процедуры Арнольди, когда матрица 
$A$ - симметричная, при таком условии хессенбергова матрица $H_m$ становится симметричной
тридиагональной $T_m$. Это позволяет получить короткие рекуррентные соотношения, 
приведённые в Алгоритме \ref{alg:lanczos} 
\begin{algorithm}
    \caption{Симметричный алгоритм Ланцоша}\label{alg:lanczos}
    \begin{algorithmic}[1]
    \State Выберем $v_1 = v / \|v\|_2$, так что $\|v_1\|_2 = 1$
    \State $\beta_1 \gets 0$, $v_0 \gets 0$
    \For{$j = 1, 2, \ldots, m$}
        \State $w_j \gets Av_j - \beta_j v_{j-1}$
        \State $\alpha_j \gets (w_j, v_j)$
        \State $w_j \gets w_j - \alpha_j v_j$
        \State $\beta_{j+1} \gets \|w_j\|_2$
        \If{$\beta_{j+1} = 0$}
            \State \textbf{Stop}
        \EndIf
        \State $v_{j+1} \gets w_j / \beta_{j+1}$
    \EndFor
    \end{algorithmic}
    \end{algorithm} 

При этом матрица $T_m$ имеет вид: 
\begin{equation}
T_m = \begin{pmatrix}
\alpha_1 & \beta_2 & & & \\
\beta_2 & \alpha_2 & \beta_3 & & \\
& \beta_3 & \ddots & \ddots & \\
& & \ddots & \alpha_{m-1} & \beta_m \\
& & & \beta_m & \alpha_m
\end{pmatrix}
\label{eq:T_m}
\end{equation}

\subsection[Метод сопряженных градиентов]{Метод сопряженных градиентов \cite{Saad2003}}
Симметричный алгоритм Ланцоша можно использовать для итеративного решения систем
линейных уравнений с симметричной положительно определенной матрицей.
\par Пусть задано начальное приближение $x_0$, и векторы направлений из алгоритма
Ланцоша $v_i,\;i=1,...,m$.
На $m$-ом шаге алгоритма приближенное решение ищется в аффинном пространстве $x_0 + K_m$, 
где $K_m\left(A,r_0\right) \equiv \{r_0,Ar_0,A^2r_0,...,A^{m-1}r_0\}, \; r_0 = b - Ax_0$.
На невязки при этом налагается условие 
\begin{equation}
    \label{eq:galerkin}
    b-Ax_m \perp K_m.
\end{equation}
Если взять $v_1 = r_0/\|r_0\|_2$ и 
обозначить $\beta = \|r_0\|_2$. Тогда $V_m^TAV_m = T_m$ из \eqref{eq:VAVHM}, а также $V_m^Tr_0 = V_m^T(\beta v_1)=\beta e_1$.
Разложим приближенное решение на $m$-ом шаге по базису из векторов $v_i,\;i=1,...,m$:
\begin{equation}
    x_m = x_0 + V_m y_m.
\end{equation} 
Это выражение эквивалентно равенству:
 \begin{equation}
    r_m = r_0 - AV_m y_m, 
 \end{equation}
домножим слева на $V_m^T$:
\begin{equation}
    \label{eq:VTreqVTr0VAVy}
    V_m^T r_m = V_m^T r_0 - V_m^T AV_m y_m.
\end{equation}
Из \eqref{eq:galerkin} следует, что $ V_m^T r_m = 0 $, учтём это в \eqref{eq:VTreqVTr0VAVy} 
и выразим $y_m$:
\begin{equation}
    y_m = T_m^{-1} \beta e_1.
\end{equation}

\par Получим выражение для $r_m$:
\begin{align*}
    r_m &= b - A(x_0 + V_m y_m) \\
        &= r_0 - AV_m y_m \\
        &= \beta v_1 - (V_m T_m + t_{m+1,m}v_{m+1}e_m^T)y_m \\
        &= V_m \underbrace{(\beta e_1 - T_m y_m)}_{=0} - t_{m+1,m} e_m^T y_m v_{m+1}
\end{align*}
\begin{equation}
    \label{eq:r_m_SL}
    r_m = - t_{m+1,m} e_m^T y_m v_{m+1}.
\end{equation}
Из этого выражения следует, что $r_m \parallel v_{m+1}$, а значит, что невязки на каждом шаге ортогональны друг другу.

\par Получим короткие итерационные соотношения для обновления приближенного решения $x_m$.
LU-разложение матрицы $T_m$:
\begin{equation*}
    T_m = L_m U_m = 
        \begin{pmatrix}
        1 & & & & \\
        \lambda_2 & 1 & & & \\
        & \lambda_3 & \ddots & & \\
        & & \ddots & 1 &  \\
        & & & \lambda_m & 1
        \end{pmatrix}
        \begin{pmatrix}
            \eta_1 & \beta_2 & & & \\
             & \eta_2 & \beta_3 & & \\
            & & \ddots & \ddots & \\
            & & & \eta_{m-1} & \beta_m \\
            & & & & \eta_m
        \end{pmatrix}
\end{equation*} 
Введем обозначения
\begin{align}
    P_m &\equiv V_m U_m^{-1}, \label{eq:P_m}\\
    z_m &\equiv L_m^{-1} \beta e_1,
\end{align}
тогда приближенное решение выражается как
\begin{equation}
    x_m = x_0 + P_m z_m.
\end{equation}
Используя равенство \eqref{eq:P_m} получим формулу для обновления $p_m$-последнего
столбца $p_m$ матрицы $P_m$
\begin{align}
    &P_m U_m = V_m \\
    &p_m \eta_m + \beta_m p_{m-1} = v_m \\
    &p_m = \eta_m^{-1} \left( v_m - \beta_m p_{m-1} \right) \label{eq:p_m_update}
\end{align}
Выразим элементы из последней строчки матрицы $T_m$ с помощью LU-разложения:
\begin{align*}
    \alpha_m = \lambda_m \beta_m + \eta_m &\implies \eta_m = \alpha_m - \lambda_m \beta_m \\
    \beta_m = \lambda_m \eta_{m-1} &\implies \lambda_m = \beta_m / \eta_{m-1}
\end{align*}
В силу вида матрицы $L_m$:
\begin{align*}
    z_m &= 
    \begin{pmatrix}
        z_{m-1} \\
        \zeta_m
    \end{pmatrix}, \\
    \zeta_m &= -\lambda_m \zeta_{m-1}. 
\end{align*}
Как результат получаем формулу для обновления $x_m$:
\begin{equation*}
    x_m = x_{m-1} + \zeta_m p_m
\end{equation*}
Покажем, что столбцы $P_m$ образуют А-ортогональную систему, т.е, что $(Ap_i,p_j) = 0$, для $i \neq j$.
Для этого нужно показать, что $P_m^T AP_m$ - диагональная матрица. Подставим определение $P_m$ в это выражение:
\begin{align}
    P_m^T AP_m &= U_m^{-T}V_m^T AV_m U_m^{-1} \\
               &= U_m^{-T}T_m U_m^{-1} \\
               &=U_m^{-T}L_m
\end{align}
$U_m^{-T}L_m$ - нижнетреугольная матрица, но она также является и симметричной, 
так как $P_m^T AP_m$ - симметричная матрица. Таким образом, $U_m^{-T}L_m$ - диагональная матрица.
\par Следствием этого является то, что обновлять приближенное решение можно исходя из
поддержания свойств ортогональности невязок и А-ортогональности векторов направлений $p_i$.
В последующий выкладках вектора $p_j$ будут нумероваться с нуля, а не с единицы, как это было раньше.
А также коэффициенты будут переименованы, чтобы соответствовать общепринятым обозначениям.
\begin{align*}
    x_{j+1} = x_j + \alpha_j p_j \implies r_{j+1} = r_j - \alpha_j A p_j \\
    \alpha_j = \left( r_j, r_j \right) / \left( Ap_j, r_j \right)
\end{align*}
Из уравнения \eqref{eq:p_m_update} после перенормировки $p_i, i=1,...,m$ следует, что 
\begin{align*}
    &p_{j+1} = r_{j+1} + \beta_j p_j  \\
    &\beta_j = - (r_{j+1}, Ap_j) / (p_j, Ap_j) = \frac{1}{\alpha_j} (r_{j+1}, (r_{j+1}-r_j)) / (Ap_j,p_j) = (r_{j+1}, r_{j+1}) / (r_j,r_j)
\end{align*}
Это выражение и А-ортогональность $p_j$ в свою очередь можно использовать, чтобы преобразовать выражение для $\alpha_j$:
\begin{align*}
    &(Ap_j,r_j) = (Ap_j,p_j-\beta_{j-1}p_{j-1}) = (Ap_j,p_j) \\
    &\alpha_j = (r_j,r_j)/(Ap_j, p_j)
\end{align*}
Теперь у нас есть всё, чтобы записать алгоритм.
\begin{algorithm}
    \caption{Метод сопряженных градиентов}
    \begin{algorithmic}[1]
    \State $r_0 \gets b - A x_0$, $p_0 \gets r_0$.
    \For{$j = 0, 1, \ldots$}
        \State $\alpha_j \gets (r_j, r_j) / (A p_j, p_j)$
        \State $x_{j+1} \gets x_j + \alpha_j p_j$
        \State $r_{j+1} \gets r_j - \alpha_j A p_j$
        \State $\beta_j \gets (r_{j+1}, r_{j+1}) / (r_j, r_j)$
        \State $p_{j+1} \gets r_{j+1} + \beta_j p_j$
    \EndFor
    \end{algorithmic}
\end{algorithm}

Этот метод можно адаптировать и для систем общего вида, если домножить обе части уравнения
$Ax=b$ на $A^T$, и решать систему с симметричной положительно определенной матрицей $A^TA$,
однако число обусловленности при этом возрастает в квадрат раз из-за чего данный вариант может 
давать плохие результаты.
 
\subsection[Процесс биортогонализации Ланцоша]{Процесс биортогонализации Ланцоша \cite{Saad2003}}
Для несимметричных систем можно предъявить алгоритм похожий на симметричный алгоритм Ланцоша,
но который будет строить не ортогональный базис в пространстве Крылова, а пару биортогональных
базисов в пространствах $K_m(A, v_1) = span\{v_1,Av_1,...,A^{m-1}v_1\}$ и $K_m(A^T, v_1) = span\{v_1,A^Tv_1,...,(A^T)^{m-1}v_1\}$, 
то есть такую пару $v_1,...,v_m$ и $w_1,...,w_m$, что $(v_i,w_j)=\delta_{ij},\;1\leq i,\;j \leq m$.
\begin{algorithm}
    \caption{Процесс биортогонализации Ланцоша}
    \begin{algorithmic}[1]
    \State Выберем $v_{1}, w_{1}$ такие что $(v_{1}, w_{1}) = 1$.
    \State $\beta_{1} = \delta_{1} \equiv 0$, $w_{0} = v_{0} \equiv 0$
    \For{$j = 1, 2, \ldots, m$}
        \State $\alpha_{j} = (A v_{j}, w_{j})$
        \State $\hat{v}_{j+1} = A v_{j} - \alpha_{j} v_{j} - \beta_{j} v_{j-1}$
        \State $\hat{w}_{j+1} = A^{T} w_{j} - \alpha_{j} w_{j} - \delta_{j} w_{j-1}$
        \State $\delta_{j+1} = |(\hat{v}_{j+1}, \hat{w}_{j+1})|^{1/2}$
        \If{$\delta_{j+1} = 0$}
            \State Stop
        \EndIf
        \State $\beta_{j+1} = (\hat{v}_{j+1}, \hat{w}_{j+1}) / \delta_{j+1}$
        \State $v_{j+1} = \hat{v}_{j+1} / \beta_{j+1}$
        \State $w_{j+1} = \hat{w}_{j+1} / \delta_{j+1}$
    \EndFor
    \end{algorithmic}
    \end{algorithm}

Введём обозначения: 
\begin{equation*}
    T_m = 
    \begin{pmatrix}
        \alpha_1 & \beta_2 & & & \\
        \delta_2 & \alpha_2 & \beta_3 & & \\
        & \delta_3 & \ddots & \ddots & \\
        & & \ddots & \alpha_{m-1} & \beta_m \\
        & & & \delta_m & \alpha_m
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    W_m = \begin{bmatrix}
            w_1 & ... & w_m
        \end{bmatrix}
\end{equation*}

Тогда легко убедиться, что если на $m$-ом шаге не произошло аварийной остановки, 
то алгоритм порождает следующие матричные соотношения:
\begin{align*}
    AV_m &= V_m T_m + \delta_{m+1} v_{m+1} e_m^T\\
    A^T W_m &= W_m T^T_m + \beta_{m+1} w_{m+1} e_m^T \\
    W_m^T A V_m &= T_m  
\end{align*}

\subsection[Метод бисопряженных градиентов]{Метод бисопряженных градиентов \cite{Saad2003}}
\par Метод бисопряженных градиентов выводится из процесса биортогонализации Ланцоша
аналогично тому, как метод сопряженных градиентов выводился из симметричного процесса
Ланцоша. Прибилиженное решение на $m$-ом шаге будет искаться как наилучшее приближение в 
пространстве $x_0 + K_m$, где $K_m = \{v_1, Av_1,...,A^{m-1}v_1\}$, так, чтобы невязка 
$r_m$ была ортогональна пространству $L_m = {w_1,A^T w_1, ... ,(A^T)^{m-1} w_1}$. Так же, 
как и при выводе сопряженных градиентов возьмём $v_1 = r_0 / \| r_0 \|_2$, а вектор 
$w_1$ можно взять произвольным, таким что $(v_1, w_1) \neq 0$, например, $v_1$. Алгоритм
будет решать не только систему $Ax=b$, но и некоторую двойственную систему $A^T x^* = b^*$ (причём
 $b^*=b$, если $w_1 = v_1$).
Производим LU-разложение для матрицы $T_m$, полученной в ходе процесса биортогонализации Ланцоша:
\begin{equation*}
    T_m = L_m U_m,
\end{equation*}
и вводим обозначения для векторов направлений $p_i$, $p_i^*$:
\begin{align*}
    P_m &= V_m U_m^{-1} \\
    P_m^* &= W_m L_m^{-T} \\
    \begin{bmatrix} p_1 & ... & p_m \end{bmatrix} &= P_m \\
    \begin{bmatrix} p_1^* & ... & p_m^* \end{bmatrix} &= P_m^* \\
\end{align*} 
Приближенное решение выражается также как и в методе сопряженных градиентов:
\begin{equation*}
    x_m = x_0 + P_m L_m^{-1} (\beta e_1).
\end{equation*}
И, аналогично методу сопряженных градиентов невязки окажутся сонаправлены векторам из базиса:
\begin{align*}
    r_j &\parallel v_{j+1},\;j=1,...,m \\
    r_j^* &\parallel v_{j+1}^*,\;j=1,...,m
\end{align*}
Отсюда следует, что эти наборы невязок биортогональны:
\begin{equation}
    \label{eq:rjrieq0}
    (r_j^*,r_i) = 0,\;\text{при}\;1 \leq i,j \leq m,\;i \neq j
\end{equation}
% Введём обозначения для невязки на $m$-ой итерации и для двойственной невязки соответственно:
% \begin{align*}
%     r_m &= b - Ax_m \\
%     r_m^* &= b^* - A^T x_m^*
% \end{align*} 
Легко показать, что наборы векторов $p_i^*,\;i=1,...,m$ и $p_i,\;i=1,...,m$ - A-ортогональны:
\begin{equation*}
    (P_m^*)^T A P_m = L_m^{-1} W_m^T A V_m U_m^{-1} = L_m^{-1} T_m U_m^{-1} = I.
\end{equation*}
Благодаря полученнным свойствам биортогональности невязок и А-биортогональности
векторов направлений, аналогичным же образом можно получить короткие итерационные соотношения для получения новых векторов 
$p_i$, $r_i$ , $x_i$, и записать окончательный алгоритм:
\begin{algorithm}
    \caption{Метод бисопряженных градиентов}
    \begin{algorithmic}[1]
    \State $r_0 \gets b - A x_0$, $r_0^*$ т.ч. $(r_0^*,r_0) \neq 0$, например $r_0^*=r_0$ 
    \State $p_0 \gets r_0$, $p_0^*=r_0^*$
    \For{$j = 0, 1, \ldots$}
        \State $\alpha_j \gets (r_j^*, r_j) / (p_j^*, A p_j)$
        \State $x_{j+1} \gets x_j + \alpha_j p_j$
        \State $r_{j+1} \gets r_j - \alpha_j A p_j$
        \State $r_{j+1}^* \gets r_j^* - \alpha_j A^T p_j^*$
        \State $\beta_j \gets (r_{j+1}^*, r_{j+1}) / (r_j^*, r_j)$
        \State $p_{j+1} \gets r_{j+1} + \beta_j p_j$
        \State $p_{j+1}^* \gets r_{j+1} + \beta_j p_j^*$
    \EndFor
    \end{algorithmic}
\end{algorithm}
К сожалению, данный метод на практике проявляет нерегулярное уменьшение невязки и 
тратит вычислительные мощности на поиск решения двойственной задачи, которая нас не интересует.
Для решения этих проблем был придуман метод стабилизированных бисопряженных градиентов.

\subsection[Стабилизированный метод бисопряженных градиентов]{Стабилизированный метод бисопряженных градиентов \cite{doi:10.1137/0913035}}
Невязки, полученные при помощи метода бисопряженных градиентов $r_k$ и $r_k^*$ лежат в пространствах
Крылова $K_{m+1}^r(A, r_0) = \{r_0,A r_0, ... , A^m r_0\}$ и \\$K_{m+1}^l(A^T, r_0^*) = \{r_0^*,(A^T) r_0^*, ... , (A^T)^m r_0^*\}$ соответственно,
следовательно, их можно выразить с помощью многочлена от матрицы:
\begin{align*}
    r_k &= \mathcal{R}_k(A)r_0 \\
    r_k^* &= \mathcal{Q}_k(A^T)r_0^*
\end{align*}
Из вида итерационных соотношений легко видеть, что $\mathcal{R_k} \equiv \mathcal{Q_k}$,
 и $\mathcal{R_k}(0) = 1$. 

 \par Как было показано в предыдущем пункте, метод бисопряженных градиентов работает
 за счёт поддержания ортогонализационных соотношений на невязки и вектора направлений.
 Преобразуем соотношение для невязок \eqref{eq:rjrieq0}:
 \begin{align*}
    (r_j^*,r_i) &= 0 \\
    (\mathcal{Q}_j(A^T)r_0^*,\mathcal{R}_i(A)r_0) &= (r_0^*,\mathcal{Q}_j(A)\mathcal{R}_i(A)r_0) = 0 
 \end{align*}
 \begin{equation}
    \label{eq:bcgstaborthresiduals}
    (r_0^*,\mathcal{Q}_j(A)\mathcal{R}_i(A)r_0) = 0
 \end{equation}

Из этого же соотношения \eqref{eq:rjrieq0} следует, что $r_j \perp K^l_{j} (A^T, r_0^*)$, 
значит, выражение \eqref{eq:bcgstaborthresiduals} верно для любого многочлена $\mathcal{Q}_j$ порядка $j$.
В частности рассмотрим 
\begin{equation}
    \label{eq:bcgstabq}
    \mathcal{Q}_j(t) = (1-\omega_0 t)(1-\omega_1 t)\cdot...\cdot(1-\omega_{j-1} t)
\end{equation}
И будем выбирать $\omega_j$ так, чтобы минимизировать норму $r_i$.
Итерационные соотношения в методе бисопряженных градиентов можно записать в полиномиальном виде:
\begin{align}
    \mathcal{R}_{i+1} &= \mathcal{R}_i - t \alpha_i \mathcal{P}_i \\
    \mathcal{P}_{i+1} &= \mathcal{P}_i + \beta_i \mathcal{P}_i,
\end{align} 
где аналогично невязкам $r_i$ вектора направлений $p_i$ были выражены через 
полином от матрицы системы как $p_i = \mathcal{P}_i(A)r_0$. 

\par Опираясь на \eqref{eq:bcgstaborthresiduals} введём обозначение для стабилизированных
невязок: 
\begin{equation}
    \label{eq:bcgstabresstab}
    r_i = \mathcal{Q}_i(A)\mathcal{R}_i(A)r_0
\end{equation}
Получим короткие итерационные соотношения для обновления стабилизированной невязки:
\begin{align*}
    &\mathcal{Q}_{i+1}(A)\mathcal{R}_{i+1}(A)r_0  = (1-\omega_iA)\mathcal{Q}_{i}(A)(\mathcal{R}_i(A)-\alpha_i \mathcal{P}_i(A))r_0 = \\
    &= \{\mathcal{Q}_{i}(A)\mathcal{R}_{i}(A) - \alpha_i A \mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)\}r_0 -\omega_iA\{ \mathcal{Q}_{i}(A)\mathcal{R}_{i}(A) - \alpha_i A \mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)\ \}r_0
\end{align*}
Обозначим $s_i = r_i - \alpha_i A p_i$, тогда обновление стабилизированной невязки будет производится по следующему соотношению:
\begin{equation}
    \label{eq:bcgstabresupdate}
    r_{i+1} = s_i - \omega_i A s_i
\end{equation}
Аналогично введем обозначение для новых векторов направлений:
\begin{equation}
    p_i = \mathcal{Q}_i(A) \mathcal{P}_i(A) r_0
\end{equation}
и получим рекуррентные соотношения для них:
\begin{align*}
    &\mathcal{Q}_{i+1}(A)\mathcal{P}_{i+1}(A)r_0 = \mathcal{Q}_{i+1}(A)(\mathcal{R}_{i+1}(A)+\beta_i\mathcal{P}_{i}(A))r_0 \\
    &= \mathcal{Q}_{i+1}(A)\mathcal{R}_{i+1}(A)r_0 + \beta_i\{\mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)r_0-\omega_iA\mathcal{Q}_{i}(A)\mathcal{P}_{i}(A)\}r_0
\end{align*}
\begin{equation}
    \label{eq:bsgstabpupdate}
    p_{i+1} = r_{i+1} + \beta_i(p_i - \omega_i Ap_i)
\end{equation}

\subsection[Блочный метод сопряженных градиентов]{Блочный метод сопряженных градиентов \cite{OLEARY1980293}}

\subsection[Блочный метод бисопряженных градиентов]{Блочный метод бисопряженных градиентов \cite{OLEARY1980293}}

\subsection[Блочный стабилизированный метод бисопряженных градиентов]{Блочный стабилизированный метод бисопряженных градиентов \cite{elGuennouni2003}}

\subsubsection{Матричнозначные полиномы}
 \par Проблемы со сходимостью метода из \cite{elGuennouni2003}, демонстрация в 4 главе, решение проблемы в 3 главе.
 \subsubsection{Алгоритм}
 \subsection[Блочный симметричный метод квазиминимальных невязок]{Блочный симметричный метод квазиминимальных невязок \cite{doi:10.1137/0917019}}


\newpage
